{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40606b79-b6b1-4de3-b23d-04ea769954e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: D:\\Python\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\python\\lib\\site-packages (from evaluate) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\25224\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: dill in d:\\python\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\python\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\python\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\python\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\python\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\python\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in d:\\python\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\python\\lib\\site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in d:\\python\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\python\\lib\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: aiohttp in d:\\python\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\python\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\python\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\python\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\python\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\python\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\python\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\python\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\python\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80893c15-ddd2-4b19-afe9-874304410a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\python\\lib\\site-packages (from sentence_transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\python\\lib\\site-packages (from sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\python\\lib\\site-packages (from sentence_transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in d:\\python\\lib\\site-packages (from sentence_transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\python\\lib\\site-packages (from sentence_transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in d:\\python\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\python\\lib\\site-packages (from sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in d:\\python\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\python\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\25224\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\python\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\python\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\python\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\python\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.2.2)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-4.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: D:\\Python\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44b718e-19cb-4aa4-b55e-86aa93e9548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\25224\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in d:\\python\\lib\\site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in d:\\python\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in d:\\python\\lib\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in d:\\python\\lib\\site-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in d:\\python\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in d:\\python\\lib\\site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.2.0)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.11.0)\n",
      "Requirement already satisfied: networkx in d:\\python\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\python\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\python\\lib\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.2.2)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.6.0 peft-0.15.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: D:\\Python\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec37b024-af77-4947-a925-852d414c8f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\python\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\25224\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\python\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.2/10.4 MB 26.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.4 MB 31.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 24.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 19.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: D:\\Python\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca06032a-a1fc-4937-ac64-6007ca4ef4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: D:\\Python\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9518be0-2748-46e9-8879-aaa92cffca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\25224\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in d:\\python\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\python\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in d:\\python\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in d:\\python\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\python\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-win_amd64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-win_amd64.whl (25.8 MB)\n",
      "   ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 4.7/25.8 MB 22.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.7/25.8 MB 23.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.1/25.8 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.6/25.8 MB 28.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.8/25.8 MB 25.1 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-win_amd64.whl (120 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-win_amd64.whl (92 kB)\n",
      "Installing collected packages: xxhash, requests, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.5.1 dill-0.3.8 frozenlist-1.6.0 huggingface-hub-0.30.2 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 requests-2.32.3 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: D:\\Python\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395daf6b-c8e0-4f3d-9cde-36379321d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement google.colab (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: D:\\Python\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for google.colab\n"
     ]
    }
   ],
   "source": [
    "!pip install google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809b879e-2eb2-40bf-9a26-07472ee82417",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Import Colab Drive for mounting\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive \u001b[38;5;66;03m# Added import for Google Drive\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m     23\u001b[0m     AutoTokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     TrainingArguments\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     LoraConfig,\n\u001b[0;32m     32\u001b[0m     get_peft_model,\n\u001b[0;32m     33\u001b[0m     TaskType,\n\u001b[0;32m     34\u001b[0m     PeftModel\n\u001b[0;32m     35\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "\n",
    "# Import DataLoader (needed by Retriever class definition, even if not used in inference)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import Dataset (needed by Generator class definition, even if not used in inference)\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import Colab Drive for mounting\n",
    "from google.colab import drive # Added import for Google Drive\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, losses, models, util, InputExample, CrossEncoder\n",
    "import evaluate\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "def load_corpus_from_squad_json(train_json_path, dev_json_path):\n",
    "    logger.info(f\"Loading corpus from local files: {train_json_path}, {dev_json_path}\")\n",
    "\n",
    "    def load_json_data(file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if 'data' not in data:\n",
    "                 logger.error(f\"JSON file {file_path} does not contain the expected 'data' key.\")\n",
    "                 return None\n",
    "            return data['data']\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred loading {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    train_data = load_json_data(train_json_path)\n",
    "    dev_data = load_json_data(dev_json_path)\n",
    "\n",
    "    if train_data is None and dev_data is None:\n",
    "         logger.error(\"Failed to load both train and dev JSON files for corpus.\")\n",
    "         return pd.DataFrame()\n",
    "\n",
    "    if train_data is None: train_data = []\n",
    "    if dev_data is None: dev_data = []\n",
    "\n",
    "    context_to_id = {}\n",
    "    documents_list = []\n",
    "    doc_id_counter = 0\n",
    "\n",
    "    def process_paragraphs_for_corpus(data_split, desc):\n",
    "        nonlocal doc_id_counter\n",
    "        if not isinstance(data_split, list): return\n",
    "        for article in tqdm(data_split, desc=f\"Processing articles ({desc} corpus)\"):\n",
    "            if not isinstance(article, dict) or 'paragraphs' not in article or not isinstance(article['paragraphs'], list): continue\n",
    "            for paragraph in article['paragraphs']:\n",
    "                if not isinstance(paragraph, dict) or 'context' not in paragraph: continue\n",
    "                context = paragraph['context']\n",
    "                if not isinstance(context, str):\n",
    "                     logger.warning(f\"Context is not a string in {desc}, type: {type(context)}. Skipping.\")\n",
    "                     continue\n",
    "                stripped_context = context.strip()\n",
    "                if stripped_context not in context_to_id:\n",
    "                    doc_id = f\"doc_{doc_id_counter}\"\n",
    "                    context_to_id[stripped_context] = doc_id\n",
    "                    documents_list.append({'id': doc_id, 'passage': context})\n",
    "                    doc_id_counter += 1\n",
    "\n",
    "    process_paragraphs_for_corpus(train_data, \"train\")\n",
    "    process_paragraphs_for_corpus(dev_data, \"dev\")\n",
    "\n",
    "    documents_df = pd.DataFrame(documents_list)\n",
    "    logger.info(f\"Loaded corpus with {len(documents_df)} unique passages.\")\n",
    "\n",
    "    if documents_df.empty:\n",
    "         logger.error(\"Corpus is empty after processing.\")\n",
    "         return pd.DataFrame()\n",
    "\n",
    "    return documents_df\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, retriever_model_name=\"all-mpnet-base-v2\", reranker_model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        logger.info(f\"Initializing retriever model: {retriever_model_name}\")\n",
    "        self.retriever_model = SentenceTransformer(retriever_model_name)\n",
    "\n",
    "        logger.info(f\"Initializing reranker model: {reranker_model_name}\")\n",
    "        self.reranker_model = CrossEncoder(reranker_model_name)\n",
    "\n",
    "        self.corpus = {}\n",
    "        self.corpus_ids = []\n",
    "        self.corpus_texts = []\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus_embeddings_device = None\n",
    "\n",
    "        self.train_examples = []\n",
    "        self.queries = {}\n",
    "        self.relevant_docs = {}\n",
    "\n",
    "    def load_corpus(self, documents_df):\n",
    "        logger.info(\"Loading corpus documents...\")\n",
    "        documents_df['id'] = documents_df['id'].astype(str)\n",
    "        documents_df['passage'] = documents_df['passage'].fillna(\"\").astype(str)\n",
    "\n",
    "        self.corpus = dict(zip(documents_df['id'], documents_df['passage']))\n",
    "        self.corpus_ids = list(self.corpus.keys())\n",
    "        self.corpus_texts = list(self.corpus.values())\n",
    "\n",
    "        logger.info(f\"Corpus size: {len(self.corpus)} documents.\")\n",
    "\n",
    "    def precompute_corpus_embeddings(self, batch_size=1024):\n",
    "        if not self.corpus_texts:\n",
    "            raise ValueError(\"No corpus found. Did you run load_corpus(...) first.\")\n",
    "\n",
    "        logger.info(f\"Computing embeddings for {len(self.corpus_texts)} passages...\")\n",
    "        all_embeddings = []\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Encoding device: {device}\")\n",
    "        self.retriever_model.to(device)\n",
    "\n",
    "        try:\n",
    "            for start_idx in tqdm(range(0, len(self.corpus_texts), batch_size), desc=\"Encoding corpus\"):\n",
    "                batch = self.corpus_texts[start_idx:start_idx + batch_size]\n",
    "                batch = [str(t) for t in batch]\n",
    "                batch_embeddings = self.retriever_model.encode(\n",
    "                    batch,\n",
    "                    convert_to_tensor=True,\n",
    "                    show_progress_bar=False,\n",
    "                    device=device\n",
    "                )\n",
    "                all_embeddings.append(batch_embeddings)\n",
    "\n",
    "            self.corpus_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "            self.corpus_embeddings_device = self.corpus_embeddings.device\n",
    "            logger.info(f\"Corpus embeddings shape: {self.corpus_embeddings.shape} on device: {self.corpus_embeddings_device}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during corpus encoding: {e}\")\n",
    "            self.corpus_embeddings = None\n",
    "            self.corpus_embeddings_device = None\n",
    "            raise\n",
    "\n",
    "    def re_rank_passages(self, query, initial_results):\n",
    "        if not initial_results: return []\n",
    "        cross_encoder_input = [[str(query), str(res['passage'])] for res in initial_results]\n",
    "        if not cross_encoder_input: return []\n",
    "\n",
    "        try:\n",
    "            rerank_scores = self.reranker_model.predict(cross_encoder_input)\n",
    "            for i, res in enumerate(initial_results):\n",
    "                res['rerank_score'] = float(rerank_scores[i])\n",
    "            reranked_results = sorted(initial_results, key=lambda x: x.get('rerank_score', -float('inf')), reverse=True)\n",
    "            return reranked_results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during re-ranking: {e}\")\n",
    "            logger.warning(\"Re-ranking failed. Returning initial results sorted by original score.\")\n",
    "            return sorted(initial_results, key=lambda x: x.get('score', -float('inf')), reverse=True)\n",
    "\n",
    "\n",
    "    def retrieve_top_k(self, query, top_k=5, initial_retrieval_k=100, use_reranking=True):\n",
    "        if self.corpus_embeddings is None or self.corpus_embeddings_device is None:\n",
    "            logger.warning(\"Corpus embeddings not precomputed or device unknown. Attempting precomputation...\")\n",
    "            try:\n",
    "                self.precompute_corpus_embeddings()\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Failed to compute corpus embeddings: {e}. Cannot perform retrieval.\")\n",
    "                 return []\n",
    "            if self.corpus_embeddings is None or self.corpus_embeddings_device is None:\n",
    "                 logger.error(\"Corpus embeddings still not available after attempted precomputation. Cannot perform retrieval.\")\n",
    "                 return []\n",
    "\n",
    "\n",
    "        device = self.corpus_embeddings_device\n",
    "        self.retriever_model.to(device)\n",
    "        query_embedding = self.retriever_model.encode(str(query), convert_to_tensor=True, device=device)\n",
    "\n",
    "        cos_scores = util.cos_sim(query_embedding, self.corpus_embeddings)[0]\n",
    "\n",
    "        initial_k = initial_retrieval_k if use_reranking else top_k\n",
    "        initial_k = min(initial_k, len(self.corpus_texts))\n",
    "        initial_k = max(1, initial_k) if len(self.corpus_texts) > 0 else 0\n",
    "\n",
    "        if initial_k == 0:\n",
    "             logger.warning(\"Corpus is empty or initial_k is 0. Cannot retrieve.\")\n",
    "             return []\n",
    "\n",
    "        top_values, top_indices = torch.topk(cos_scores, k=initial_k)\n",
    "        top_indices_list = top_indices.cpu().numpy()\n",
    "\n",
    "        initial_retrieved_passages = []\n",
    "        corpus_ids_str = [str(cid) for cid in self.corpus_ids]\n",
    "\n",
    "        for idx in top_indices_list:\n",
    "            cid = corpus_ids_str[idx]\n",
    "            passage = self.corpus.get(cid, \"\")\n",
    "            initial_retrieved_passages.append({\n",
    "                'corpus_id': cid,\n",
    "                'passage': passage,\n",
    "                'score': cos_scores[idx].item()\n",
    "            })\n",
    "\n",
    "        if use_reranking:\n",
    "            reranked_results = self.re_rank_passages(query, initial_retrieved_passages)\n",
    "            return reranked_results[:top_k]\n",
    "        else:\n",
    "            return sorted(initial_retrieved_passages, key=lambda x: x.get('score', -float('inf')), reverse=True)[:top_k]\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    pass\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, retriever_model_path=None, generator_model_path=None,\n",
    "                 retriever_base_model=\"all-mpnet-base-v2\", reranker_base_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "                 generator_base_model=\"gpt2\"):\n",
    "        logger.info(\"Initializing RAG System...\")\n",
    "\n",
    "        if retriever_model_path and os.path.exists(retriever_model_path):\n",
    "             logger.info(f\"Loading trained retriever model from {retriever_model_path}\")\n",
    "             try:\n",
    "                 self.retriever = Retriever(retriever_model_name=retriever_model_path, reranker_model_name=reranker_base_model)\n",
    "                 logger.info(\"Trained Retriever model loaded.\")\n",
    "             except Exception as e:\n",
    "                 logger.error(f\"Failed to load trained retriever model from {retriever_model_path}: {e}\")\n",
    "                 logger.warning(f\"Proceeding by loading base retriever model {retriever_base_model}.\")\n",
    "                 self.retriever = Retriever(retriever_base_model, reranker_base_model)\n",
    "        else:\n",
    "             logger.warning(f\"Retriever model path not provided or does not exist: {retriever_model_path}\")\n",
    "             logger.info(f\"Loading base retriever model {retriever_base_model}.\")\n",
    "             self.retriever = Retriever(retriever_base_model, reranker_base_model)\n",
    "\n",
    "\n",
    "        logger.info(f\"Loading generator model from {generator_model_path}...\")\n",
    "\n",
    "        latest_checkpoint = None\n",
    "        if generator_model_path and os.path.exists(generator_model_path):\n",
    "            checkpoint_dirs = glob.glob(os.path.join(generator_model_path, 'checkpoint-*'))\n",
    "            if checkpoint_dirs:\n",
    "                try:\n",
    "                    latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(os.path.basename(x).split('-')[1]))\n",
    "                    logger.info(f\"Found latest checkpoint: {latest_checkpoint}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not determine latest checkpoint from names: {e}. Using the main generator_model_path directly if it contains model files.\")\n",
    "                    latest_checkpoint = generator_model_path\n",
    "\n",
    "\n",
    "        model_load_path = latest_checkpoint if latest_checkpoint and os.path.exists(latest_checkpoint) else generator_base_model\n",
    "\n",
    "        try:\n",
    "            self.generator_model = AutoModelForCausalLM.from_pretrained(model_load_path)\n",
    "            self.generator_tokenizer = AutoTokenizer.from_pretrained(model_load_path)\n",
    "            logger.info(f\"Generator model loaded from {model_load_path}.\")\n",
    "\n",
    "            if self.generator_tokenizer.pad_token is None:\n",
    "                 self.generator_tokenizer.pad_token = self.generator_tokenizer.eos_token\n",
    "            self.generator_model.config.pad_token_id = self.generator_tokenizer.eos_token_id\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load generator model from {model_load_path}: {e}\")\n",
    "            logger.warning(f\"Proceeding by loading base generator model {generator_base_model}.\")\n",
    "            try:\n",
    "                self.generator_model = AutoModelForCausalLM.from_pretrained(generator_base_model)\n",
    "                self.generator_tokenizer = AutoTokenizer.from_pretrained(generator_base_model)\n",
    "                if self.generator_tokenizer.pad_token is None:\n",
    "                     self.generator_tokenizer.pad_token = self.generator_tokenizer.eos_token\n",
    "                self.generator_model.config.pad_token_id = self.generator_tokenizer.eos_token_id\n",
    "                logger.info(f\"Base generator model {generator_base_model} loaded instead.\")\n",
    "            except Exception as base_e:\n",
    "                logger.error(f\"Failed to load base generator model {generator_base_model} either: {base_e}\")\n",
    "                self.generator_model = None\n",
    "                self.generator_tokenizer = None\n",
    "                logger.error(\"Generator model could not be loaded. Generation will not work.\")\n",
    "\n",
    "\n",
    "        self.corpus_doc_dict = {}\n",
    "\n",
    "\n",
    "    def load_corpus(self, documents_df):\n",
    "        logger.info(\"Loading corpus into RAG system...\")\n",
    "        if documents_df.empty:\n",
    "             logger.warning(\"Corpus DataFrame is empty.\")\n",
    "             self.corpus_doc_dict = {}\n",
    "             self.retriever.corpus = {}\n",
    "             self.retriever.corpus_ids = []\n",
    "             self.retriever.corpus_texts = []\n",
    "        else:\n",
    "            documents_df[\"id\"] = documents_df[\"id\"].astype(str)\n",
    "            documents_df[\"passage\"] = documents_df[\"passage\"].fillna(\"\").astype(str)\n",
    "            self.corpus_doc_dict = dict(zip(documents_df[\"id\"], documents_df[\"passage\"]))\n",
    "            self.retriever.load_corpus(documents_df)\n",
    "            logger.info(\"Corpus loaded into RAG system's internal storage.\")\n",
    "\n",
    "\n",
    "    def precompute_retriever_embeddings(self, batch_size=1024):\n",
    "        if not self.retriever.corpus_texts:\n",
    "             logger.warning(\"Retriever corpus is empty. Cannot precompute embeddings.\")\n",
    "             return\n",
    "\n",
    "        logger.info(\"Starting precomputation of retriever embeddings...\")\n",
    "        try:\n",
    "            self.retriever.precompute_corpus_embeddings(batch_size=batch_size)\n",
    "            logger.info(\"Retriever embeddings precomputation complete.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Retriever embeddings precomputation failed: {e}\")\n",
    "\n",
    "\n",
    "    def answer_question(self, question: str, top_k_retrieval=5, initial_retrieval_k=100, use_reranking=True,\n",
    "                        max_new_tokens=100, do_sample=False):\n",
    "        if self.retriever.corpus_embeddings is None or self.retriever.corpus_embeddings_device is None:\n",
    "            logger.error(\"Retriever corpus embeddings not available. Cannot perform retrieval.\")\n",
    "            return \"Error: Retriever embeddings not loaded or failed to compute.\"\n",
    "\n",
    "        if not self.corpus_doc_dict:\n",
    "             logger.error(\"Corpus document dictionary not loaded. Cannot retrieve passage texts.\")\n",
    "             return \"Error: Corpus not loaded.\"\n",
    "\n",
    "        if self.generator_model is None or self.generator_tokenizer is None:\n",
    "             logger.error(\"Generator model or tokenizer not loaded. Cannot perform generation.\")\n",
    "             return \"Error: Generator model not loaded.\"\n",
    "\n",
    "\n",
    "        logger.info(f\"Processing question: '{question}'\")\n",
    "\n",
    "        logger.info(f\"Retrieving top {top_k_retrieval} passages...\")\n",
    "        retrieved_results = self.retriever.retrieve_top_k(\n",
    "            question,\n",
    "            top_k=top_k_retrieval,\n",
    "            initial_retrieval_k=initial_retrieval_k,\n",
    "            use_reranking=use_reranking\n",
    "        )\n",
    "\n",
    "        if not retrieved_results:\n",
    "            logger.warning(\"No passages retrieved.\")\n",
    "            combined_context = \"\"\n",
    "            logger.info(\"Generating with empty context.\")\n",
    "        else:\n",
    "            combined_context = \" \".join([self.corpus_doc_dict.get(res.get('corpus_id', ''), '') for res in retrieved_results])\n",
    "\n",
    "            if not combined_context.strip():\n",
    "                 logger.warning(\"Retrieved passages are empty or contain no text after lookup.\")\n",
    "                 combined_context = \"\"\n",
    "                 logger.info(\"Generating with empty context.\")\n",
    "            else:\n",
    "                 logger.info(f\"Combined context length: {len(combined_context)} characters.\")\n",
    "\n",
    "\n",
    "        logger.info(\"Generating answer...\")\n",
    "\n",
    "        model = self.generator_model\n",
    "        tokenizer = self.generator_tokenizer\n",
    "        max_positions = model.config.max_position_embeddings\n",
    "\n",
    "        max_new_tokens_gen = max_new_tokens\n",
    "        allowed_prompt_length = max_positions - max_new_tokens_gen\n",
    "        allowed_prompt_length = max(1, allowed_prompt_length)\n",
    "\n",
    "\n",
    "        q_part_template = f\"Question: {question}\\nContext: \"\n",
    "        a_part_template = \"\\nAnswer: \"\n",
    "\n",
    "        q_tokens_len = len(tokenizer.encode(q_part_template, add_special_tokens=False))\n",
    "        a_tokens_len = len(tokenizer.encode(a_part_template, add_special_tokens=False))\n",
    "\n",
    "        fixed_length = q_tokens_len + a_tokens_len\n",
    "        allowed_for_context = max(0, allowed_prompt_length - fixed_length)\n",
    "\n",
    "        context_to_truncate = combined_context\n",
    "\n",
    "        if not isinstance(context_to_truncate, str) or not context_to_truncate.strip():\n",
    "             truncated_context = \"\"\n",
    "        else:\n",
    "             context_tokens = tokenizer.encode(context_to_truncate, add_special_tokens=False)\n",
    "             truncated_context_tokens = context_tokens[:allowed_for_context]\n",
    "             truncated_context = tokenizer.decode(truncated_context_tokens, skip_special_tokens=True)\n",
    "\n",
    "        prompt = f\"Question: {question}\\nContext: {truncated_context}\\nAnswer: \"\n",
    "\n",
    "        final_prompt_length = len(tokenizer.encode(prompt, add_special_tokens=True))\n",
    "        if final_prompt_length > allowed_prompt_length:\n",
    "             logger.warning(f\"Final prompt length ({final_prompt_length}) exceeds allowed ({allowed_prompt_length}) before generation.\")\n",
    "             prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "             truncated_prompt_tokens = prompt_tokens[:allowed_prompt_length]\n",
    "             prompt = tokenizer.decode(truncated_prompt_tokens, skip_special_tokens=True)\n",
    "             logger.warning(f\"Further truncated prompt to {len(tokenizer.encode(prompt, add_special_tokens=True))} tokens.\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            model.eval()\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            model.to(device)\n",
    "\n",
    "\n",
    "            gen_pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=0 if device == \"cuda\" else -1,\n",
    "            )\n",
    "\n",
    "            generation_params = {\n",
    "                \"max_new_tokens\": max_new_tokens_gen,\n",
    "                \"num_return_sequences\": 1,\n",
    "                \"pad_token_id\": tokenizer.eos_token_id,\n",
    "                \"eos_token_id\": tokenizer.eos_token_id,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "            if do_sample:\n",
    "                generation_params[\"do_sample\"] = True\n",
    "                generation_params[\"temperature\"] = 0.7\n",
    "                generation_params[\"top_k\"] = 50\n",
    "                generation_params[\"top_p\"] = 0.95\n",
    "            else:\n",
    "                 generation_params[\"do_sample\"] = False\n",
    "\n",
    "            gen_output = gen_pipe(\n",
    "                prompt,\n",
    "                **generation_params\n",
    "            )\n",
    "            logger.debug(f\"Raw pipeline output: {gen_output}\")\n",
    "\n",
    "\n",
    "            if gen_output and isinstance(gen_output, list) and len(gen_output) > 0 and 'generated_text' in gen_output[0]:\n",
    "                 generated_text = gen_output[0]['generated_text'].strip()\n",
    "                 if \"Answer: \" in generated_text:\n",
    "                      generated_text = generated_text.split(\"Answer: \")[-1].strip()\n",
    "                 elif \"Answer:\" in generated_text:\n",
    "                      generated_text = generated_text.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "\n",
    "            else:\n",
    "                 generated_text = \"\"\n",
    "                 logger.warning(f\"Pipeline returned unexpected output for prompt starting '{prompt[:50]}...'. Output: {gen_output}\")\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                 torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "            final_answer = generated_text\n",
    "\n",
    "            return final_answer\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during generation: {e}\")\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "            return \"Error: Could not generate answer.\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Colab Specific Setup ---\n",
    "    # Mount Google Drive to access data and models\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        logger.info(\"Google Drive mounted successfully.\")\n",
    "        # Define base path for data and models within your Drive\n",
    "        # *** UPDATE THIS PATH TO WHERE YOUR train_data.json, dev_data.json, and output folder are stored in Drive ***\n",
    "        # Example: If your files are in 'My Drive/Colab_RAG_Project/'\n",
    "        drive_base_path = '/content/drive/MyDrive/Colab_RAG_Project' # <-- UPDATE THIS PATH\n",
    "        train_json_file = os.path.join(drive_base_path, 'train_data.json')\n",
    "        dev_json_file = os.path.join(drive_base_path, 'dev_data.json')\n",
    "        retriever_model_path = os.path.join(drive_base_path, \"output/retriever-model\")\n",
    "        generator_model_path = os.path.join(drive_base_path, \"output/generator-finetuned\")\n",
    "\n",
    "        # Ensure output directories exist (within Drive)\n",
    "        os.makedirs(retriever_model_path, exist_ok=True)\n",
    "        os.makedirs(generator_model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to mount Google Drive or set paths: {e}\")\n",
    "        logger.warning(\"Proceeding with local paths. Ensure data and models are in the local Colab environment.\")\n",
    "        # Fallback to local paths if Drive mounting fails\n",
    "        train_json_file = 'train_data.json'\n",
    "        dev_json_file = 'dev_data.json'\n",
    "        retriever_model_path = \"output/retriever-model\"\n",
    "        generator_model_path = \"output/generator-finetuned\"\n",
    "\n",
    "        # Ensure local output directories exist\n",
    "        os.makedirs(retriever_model_path, exist_ok=True)\n",
    "        os.makedirs(generator_model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    # --- Check for GPU Availability ---\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        # Note: Selecting the specific GPU type (like A100) is done in Colab's Runtime settings:\n",
    "        # Runtime -> Change runtime type -> Hardware accelerator -> GPU -> Choose GPU type (e.g., A100)\n",
    "    else:\n",
    "        logger.warning(\"CUDA is not available. Using CPU. This will be significantly slower.\")\n",
    "\n",
    "\n",
    "    # Define base models (needed for loading if trained models not found/fail to load)\n",
    "    retriever_base_model = \"all-mpnet-base-v2\"\n",
    "    reranker_base_model = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    generator_base_model = \"gpt2\" # Base GPT-2 model name\n",
    "\n",
    "\n",
    "    # --- Load Corpus ---\n",
    "    logger.info(\"--- Loading Corpus Data for Inference ---\")\n",
    "    corpus_documents_df = load_corpus_from_squad_json(train_json_file, dev_json_file)\n",
    "\n",
    "    if corpus_documents_df.empty:\n",
    "        logger.error(\"Corpus loading failed or resulted in empty documents_df. Cannot run RAG system. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    logger.info(\"--- Corpus Loading Complete ---\")\n",
    "\n",
    "    # --- Initialize and Setup RAG System ---\n",
    "    logger.info(\"--- Initializing RAG System ---\")\n",
    "    try:\n",
    "        rag_system = RAGSystem(\n",
    "            retriever_model_path=retriever_model_path,\n",
    "            generator_model_path=generator_model_path, # Pass the directory containing checkpoint folders\n",
    "            retriever_base_model=retriever_base_model,\n",
    "            reranker_base_model=reranker_base_model,\n",
    "            generator_base_model=generator_base_model # Pass the base model name\n",
    "        )\n",
    "\n",
    "        rag_system.load_corpus(corpus_documents_df)\n",
    "\n",
    "        logger.info(\"Precomputing retriever embeddings for RAG inference...\")\n",
    "        # Adjust batch size based on your GPU memory (A100 can handle larger batches)\n",
    "        rag_system.precompute_retriever_embeddings(batch_size=2048) # Increased batch size example\n",
    "        if rag_system.retriever.corpus_embeddings is None:\n",
    "            logger.error(\"Retriever embeddings precomputation failed. Cannot run RAG system. Exiting.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize or setup RAG System: {e}. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    logger.info(\"--- RAG System Ready ---\")\n",
    "    print(\"\\nType your question and press Enter.\")\n",
    "    print(\"Type 'quit' or 'exit' to leave the chat.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # --- Command-Line Interface Loop ---\n",
    "    while True:\n",
    "        try:\n",
    "            user_question = input(\"Enter your question: \")\n",
    "\n",
    "            if user_question.lower() in ['quit', 'exit']:\n",
    "                print(\"Exiting chat. Goodbye!\")\n",
    "                break\n",
    "\n",
    "            if not user_question.strip():\n",
    "                 print(\"Please enter a question.\")\n",
    "                 continue\n",
    "\n",
    "            answer = rag_system.answer_question(\n",
    "                user_question,\n",
    "                top_k_retrieval=5, # Number of passages to retrieve\n",
    "                initial_retrieval_k=50, # Candidates before reranking\n",
    "                use_reranking=True, # Use reranking\n",
    "                max_new_tokens=150, # Max length of the generated answer\n",
    "                do_sample=False # Set to True for less deterministic answers\n",
    "            )\n",
    "\n",
    "            print(f\"\\nAnswer: {answer}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        except EOFError:\n",
    "            print(\"\\nExiting chat. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during answering: {e}\")\n",
    "            print(\"An error occurred while processing your question. Please try again.\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # Optional: Clean up models from memory/GPU when exiting\n",
    "    if torch.cuda.is_available():\n",
    "        if hasattr(rag_system, 'retriever') and hasattr(rag_system.retriever, 'retriever_model') and rag_system.retriever.retriever_model is not None:\n",
    "             try: rag_system.retriever.retriever_model.cpu()\n",
    "             except: pass\n",
    "             if hasattr(rag_system.retriever, 'reranker_model'):\n",
    "                 try: rag_system.retriever.reranker_model.model.to('cpu')\n",
    "                 except: pass\n",
    "        if hasattr(rag_system, 'generator_model') and rag_system.generator_model is not None:\n",
    "            try: rag_system.generator_model.to('cpu')\n",
    "            except: pass\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
